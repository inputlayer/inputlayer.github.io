1:"$Sreact.fragment"
2:I[8014,["758","static/chunks/758-3cb69ce377cde046.js","830","static/chunks/830-dbae36b1030f3d7c.js","74","static/chunks/app/use-cases/page-b59e57c47d30c74d.js"],"UseCasesIndexClient"]
7:I[484,[],"OutletBoundary"]
8:"$Sreact.suspense"
3:T1ae6,
# Agentic AI

If you're building AI agents, you've probably hit the point where vector search isn't enough. Your agent can recall things that *look* relevant, but it can't actually reason about them. It can't follow a chain of facts, enforce access policies, or keep its memory consistent when things change.

That's the gap InputLayer fills. It gives your agents a structured knowledge graph with deductive reasoning, vector search, and incremental computation - all in a single system that sits alongside your existing tools.

## Where current agent architectures fall short

Most agent frameworks treat memory as a retrieval problem. You embed observations into vectors, store them, and retrieve by similarity when the agent needs context. This works well for simple recall - "What did the user say about their preferences?" - and it's the right tool for that job.

But it breaks down when your agent needs to actually think through a problem.

```chain
Agent is asked: "Which suppliers are affected by the port closure?"
-- needs to know
Which suppliers exist and which ports they use
-- needs to know
Which ports are currently closed
-- needs to connect these facts
No single document contains this answer [highlight]
=> The agent must traverse a chain of relationships to get there
```

**Multi-hop questions** like this are the most common challenge. Three separate pieces of information need to be connected. No single document or embedding contains the answer - the agent needs to follow the chain.

**Policy enforcement** comes up when an agent is asked "Show me all documents I'm allowed to see about Project X." It needs to resolve authorization hierarchies: who reports to whom, which teams have access, what classification levels exist. This is logical reasoning, not similarity matching.

**Consistency** is the sneaky one. When an agent learns that a previous fact was wrong, all conclusions derived from that fact should disappear. With vector stores, stale observations just sit there forever unless you manually clean them up.

```steps
Agent learns: "Supplier X ships through Shanghai port" :: Stored as a fact
Agent learns: "Shanghai port is closed" :: Stored as a fact
Engine derives: "Supplier X is disrupted" :: Automatically connected [success]
Agent learns: "Shanghai port reopened" :: Fact retracted
Engine retracts: "Supplier X is disrupted" :: Automatically cleaned up [highlight]
```

## Agent memory as a knowledge graph

Instead of storing memories as unstructured text chunks, InputLayer lets you represent them as structured facts and rules. The difference is fundamental: your agent can reason about its knowledge, not just search through it.

```tree
Traditional agent memory [muted]
  Vector store
    "Customer Acme is enterprise tier" (text chunk)
    "Acme contract is $150K" (text chunk)
    "Acme renewal is March 2026" (text chunk)
```

```tree
InputLayer agent memory [primary]
  Knowledge graph
    customer(acme, enterprise, $150K)
    renewal(acme, 2026-03-15)
    Rule: high-value if enterprise + contract > $100K
    Rule: churn-risk if high-value + renewal within 90 days
    Derived: acme is high-value [success]
    Derived: acme is at churn risk [highlight]
```

With vector memory, the agent stores text chunks. It can recall them by similarity, but it can't combine them to reach conclusions.

With InputLayer, the agent stores structured facts and defines rules. The engine evaluates those rules automatically. When the agent stores a new fact - say it learns a customer's renewal is coming up - all derived conclusions update. When it retracts a fact - say the customer renews - stale conclusions vanish on their own.

## Tool-use orchestration

Agents that use tools generate chains of observations and actions. InputLayer tracks these chains as structured relationships, which lets the agent reason about what it's learned so far and what it should investigate next.

```chain
Agent calls search API
-- learns
Port closure in Shanghai [highlight]
-- knowledge graph already knows
Supplier A ships through Shanghai
-- engine automatically derives
Supplier A is disrupted [highlight]
=> Agent didn't need to manually connect these dots [success]
```

Each new fact feeds into the existing web of relationships, potentially triggering new derived conclusions without the agent having to explicitly connect the dots. The more tool calls the agent makes, the richer the reasoning becomes.

## Multi-agent coordination

When multiple agents share a knowledge graph, they can coordinate through shared state instead of complex message-passing protocols.

```flow
Agent A: detects negative sentiment [primary] -> Shared knowledge graph -> Agent B: detects usage decline [primary]
```

```note
type: tip
Neither agent had to tell the other anything. Both contributed facts. A shared rule - "churn risk if negative sentiment AND usage drop > 30%" - derived the conclusion from the combined knowledge. No pub/sub, no event buses, no eventual consistency headaches.
```

## Policy-aware retrieval

Every query can combine vector similarity with logical access control. This is particularly useful for agents that serve multiple users with different permission levels.

When an agent searches for documents on behalf of a user, InputLayer evaluates the authorization rules as part of the query itself - not as a separate middleware layer. The engine resolves the user's permission chain (following reporting hierarchies, team memberships, classification levels) and combines it with semantic similarity in a single pass. Results come back already filtered for what the user is allowed to see.

This means access control is always consistent and never stale. When someone changes roles, the very next query reflects their new permissions.

## Why teams add InputLayer for their agents

The short version: your vector database handles similarity search well, and you should keep using it for that. InputLayer adds the capabilities it can't provide - multi-hop reasoning, recursive access control, automatic retraction, incremental updates, and structured memory that your agents can actually think with.

Most teams start by using InputLayer for the queries that require reasoning, while keeping their existing vector store for straightforward similarity search. Over time, they often find themselves running more and more of their logic through the knowledge graph as the benefits of structured reasoning become clear.

## Getting started

```bash
docker run -p 8080:8080 ghcr.io/inputlayer/inputlayer
```

The [quickstart guide](/docs/guides/quickstart/) will get you running in about 5 minutes. From there, the [Python SDK](/docs/guides/python-sdk/) is the fastest way to integrate with your agent framework, and the [data modeling guide](/docs/guides/core-concepts/) will help you design a knowledge graph schema that fits your use case.4:T15a8,
# Financial Services

Financial compliance is fundamentally a reasoning problem. Whether you're screening transactions against sanctions lists, tracing beneficial ownership through layers of corporate structure, or monitoring for suspicious activity patterns, the answers live in the connections between entities - not in any single document or database record.

InputLayer adds a reasoning layer that follows these entity chains, evaluates compliance rules, and keeps derived assessments up to date as the underlying facts change.

## Sanctions screening

The basic version of sanctions screening is straightforward: check whether a transaction counterparty appears on a sanctions list. Most compliance teams have this covered. The hard part is indirect exposure - when a sanctioned person controls an entity through layers of corporate ownership.

```chain
Your client sends $50K to Alpha Corp
-- subsidiary of
Beta LLC
-- 60% owned by
Gamma Holding
-- 80% owned by
SANCTIONED ENTITY [highlight]
=> Each entity looks clean in isolation. The violation is only visible through the chain.
```

InputLayer handles this with recursive reasoning. You describe the rule: "An entity has sanctions exposure if it's directly sanctioned, or if it's owned above a certain threshold by an entity that has sanctions exposure." The engine follows ownership chains to any depth, checking at every level.

```steps
New sanctions designation published :: Add the fact to InputLayer
Engine traces all ownership chains :: Finds every entity connected to the sanctioned person [primary]
Downstream assessments update automatically :: Affected counterparties flagged in milliseconds [highlight]
```

On the flip side, when someone is removed from a sanctions list, all the downstream flags clear automatically - no manual cleanup needed.

## Beneficial ownership

Regulators around the world are tightening beneficial ownership requirements. The core question: who are the natural persons that ultimately own or control this entity?

```flow
Person X (80%) -> Holding A (60%) -> Company B [primary]
```

```note
type: info
Effective beneficial ownership: 80% x 60% = 48%. If your regulatory threshold is 25%, Person X is a beneficial owner of Company B even though they don't own it directly.
```

InputLayer computes these percentages through any number of corporate layers automatically. The engine handles the multiplication and propagation. Add more layers and the math compounds:

```flow
Person X (80%) -> Holding A (60%) -> Sub B (70%) -> Company C [primary]
```

Effective ownership: 80% x 60% x 70% = 33.6%. Still above 25%.

You define a minimum threshold, and InputLayer identifies every natural person who qualifies as a beneficial owner for every entity in your graph. When corporate structures change - new acquisitions, divestitures, ownership transfers - only the affected calculations recompute.

## Transaction monitoring

Beyond sanctions screening, compliance teams need to identify suspicious patterns across transaction flows. Take structuring as an example - splitting a large transaction into smaller ones to avoid reporting thresholds.

```tree
Sanctioned Person [highlight]
  Entity A
  Entity B
  Entity C
```

```steps
Entity A sends $4,000 to Target Company :: Below $10K threshold - looks clean
Entity B sends $3,500 to Target Company :: Below $10K threshold - looks clean
Entity C sends $3,000 to Target Company :: Below $10K threshold - looks clean
Combined total: $10,500 :: Above threshold - ALERT [highlight]
```

Each individual transaction is below $10,000. But the entities are related through common ownership, and their combined transactions exceed the threshold.

InputLayer's recursive reasoning identifies these relationships automatically. It determines which entities are connected through any chain of ownership - not just direct connections, but indirect ones through any number of intermediaries. Then it aggregates transactions from all related entities within a time window.

The key insight: the "related entity" determination is itself a recursive traversal. Entity A owns B, B owns C, so A and C are related even though there's no direct connection. Traditional transaction monitoring systems that only check direct counterparties would miss this entirely.

## Why incremental computation matters for compliance

Financial data changes constantly. New transactions arrive, entity relationships are updated, sanctions lists are revised.

```flow
Batch approach: Sanctions list updated [highlight] -> Full recomputation (minutes) -> Alerts stale until done
```

```flow
InputLayer: Sanctions list updated [success] -> Incremental update (milliseconds) -> Alerts current immediately
```

When a new transaction arrives, only the affected monitoring rules recompute. When an ownership structure changes, only the beneficial ownership calculations involving the changed entities update.

The correct retraction property is equally important. When an entity is removed from a sanctions list, all the downstream flags derived from that designation clear automatically. This prevents your compliance team from chasing alerts that are no longer valid.

## Getting started

If you're working on compliance or transaction monitoring, the [quickstart guide](/docs/guides/quickstart/) is the fastest way to start exploring. The [recursion documentation](/docs/guides/recursion/) is particularly relevant since most compliance rules are recursive in nature.

```bash
docker run -p 8080:8080 ghcr.io/inputlayer/inputlayer
```5:T1298,
# Retail & Commerce AI

If you're building AI for retail or e-commerce, you're probably familiar with the gap between what your recommendation engine suggests and what would actually be the right answer. The product that's semantically similar to what a customer bought isn't always the product they should buy next.

InputLayer helps bridge that gap by adding a reasoning layer that understands product relationships, customer behavior patterns, and business rules - things that similarity search alone can't capture.

## The recommendation problem

Traditional recommendation engines rely on similarity - collaborative filtering ("users who bought X bought Y") or content-based filtering ("products that look like X"). Both miss the same thing: logical relationships between products.

```chain
Customer buys a DSLR camera
-- similarity-based system recommends
More cameras (semantically similar) [highlight]
-- but what they actually need
Lens, memory card, camera bag (accessories) [success]
=> The connection is logical, not semantic
```

InputLayer lets you express these relationships as rules. "When a customer buys a product, recommend its accessories - but only if they haven't already bought them and they're currently in stock."

```tree
Recommendation signals [primary]
  Collaborative filtering
    "users who bought X also bought Y"
  Category affinity (recursive)
    "related product categories, at any depth"
  Semantic similarity
    "products with similar descriptions"
  Accessory relationships
    "this product goes with that one"
```

The nice thing about expressing recommendations as rules is that they're auditable. When a recommendation shows up, you can trace exactly why. That kind of explainability is hard to get from black-box models.

## Catalog reasoning

Large product catalogs have rich internal structure that's hard to capture in vector embeddings.

```tree
Sports [primary]
  Athletic
    Footwear
      Running Shoes
      Trail Shoes
    Accessories
      Running Socks
      Hydration Pack
    Electronics
      GPS Watch
```

A query like "show me all products in the athletic category" requires traversing this hierarchy. A flat metadata filter won't do that. InputLayer's recursive reasoning follows the parent-child chain and returns products from every subcategory, no matter how deep.

Cross-category recommendations also become natural. If customers who buy running shoes frequently also buy running socks (a different category), InputLayer can capture that as a rule and surface it as a cross-sell recommendation.

## Conversational commerce

Chatbots and conversational agents for e-commerce need to reason about products in context. When a customer says "I need something waterproof for hiking under $100," the agent needs to combine attribute filtering (waterproof, hiking-appropriate), price constraints, and inventory availability - potentially across thousands of products.

InputLayer handles this by letting you express all of these constraints in a single query. The engine filters by structured attributes (waterproof, suitable for hiking), applies price constraints, checks inventory status, and can even layer in semantic similarity to capture nuances the structured attributes might miss - all in one pass.

The result is products that match both the explicit requirements and the implicit intent behind the customer's question. And because everything runs through the same reasoning engine, adding a new constraint (like "also factor in the customer's past purchases") is just another rule.

## Keeping recommendations fresh

When a product goes out of stock, gets discontinued, or has a price change, recommendations should reflect that immediately.

```flow
Traditional ML [highlight] -> Retrain model (hours) -> Rebuild index (minutes) -> Deploy
```

```flow
InputLayer [success] -> Retract fact -> Recommendations update (~ms) -> Done
```

InputLayer's incremental computation handles this naturally. Update a product's stock status, and every recommendation that depended on it being in stock updates automatically. No batch job, no cache invalidation, no delay.

When a new product arrives and you add facts about it, it immediately becomes eligible for recommendation through all existing rules. No model retraining or index rebuilding needed.

## Getting started

The [quickstart guide](/docs/guides/quickstart/) will get you up and running in about 5 minutes. From there, the [data modeling guide](/docs/guides/core-concepts/) covers how to structure your product catalog as a knowledge graph, and the [Python SDK](/docs/guides/python-sdk/) makes it straightforward to integrate with your existing e-commerce platform.

```bash
docker run -p 8080:8080 ghcr.io/inputlayer/inputlayer
```0:{"buildId":"05bjYGyiAumoEMBrfWHih","rsc":["$","$1","c",{"children":[["$","$L2",null,{"useCases":[{"slug":"agentic-ai","title":"Agentic AI","icon":"Brain","subtitle":"Give your AI agents structured memory, multi-hop reasoning, and policy-aware retrieval.","content":"$3","toc":[{"level":2,"text":"Where current agent architectures fall short","id":"where-current-agent-architectures-fall-short"},{"level":2,"text":"Agent memory as a knowledge graph","id":"agent-memory-as-a-knowledge-graph"},{"level":2,"text":"Tool-use orchestration","id":"tool-use-orchestration"},{"level":2,"text":"Multi-agent coordination","id":"multi-agent-coordination"},{"level":2,"text":"Policy-aware retrieval","id":"policy-aware-retrieval"},{"level":2,"text":"Why teams add InputLayer for their agents","id":"why-teams-add-inputlayer-for-their-agents"},{"level":2,"text":"Getting started","id":"getting-started"}]},{"slug":"financial-services","title":"Financial Services","icon":"Shield","subtitle":"Sanctions screening, beneficial ownership chains, and transaction monitoring through entity reasoning.","content":"$4","toc":[{"level":2,"text":"Sanctions screening","id":"sanctions-screening"},{"level":2,"text":"Beneficial ownership","id":"beneficial-ownership"},{"level":2,"text":"Transaction monitoring","id":"transaction-monitoring"},{"level":2,"text":"Why incremental computation matters for compliance","id":"why-incremental-computation-matters-for-compliance"},{"level":2,"text":"Getting started","id":"getting-started"}]},{"slug":"retail-commerce-ai","title":"Retail & Commerce AI","icon":"ShoppingBag","subtitle":"Product recommendations, catalog reasoning, and conversational commerce powered by knowledge graphs.","content":"$5","toc":[{"level":2,"text":"The recommendation problem","id":"the-recommendation-problem"},{"level":2,"text":"Catalog reasoning","id":"catalog-reasoning"},{"level":2,"text":"Conversational commerce","id":"conversational-commerce"},{"level":2,"text":"Keeping recommendations fresh","id":"keeping-recommendations-fresh"},{"level":2,"text":"Getting started","id":"getting-started"}]}]}],null,"$L6"]}],"loading":null,"isPartial":false}
6:["$","$L7",null,{"children":["$","$8",null,{"name":"Next.MetadataOutlet","children":"$@9"}]}]
9:null
